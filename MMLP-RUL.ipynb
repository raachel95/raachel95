{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UbnQYAVoK-UN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout, Conv1D\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras_self_attention\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import TensorBoard\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NK-mrVtPLmGM"
      },
      "outputs": [],
      "source": [
        "# from keras.utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import LSTM \n",
        "from keras.layers import Attention\n",
        "from keras.layers import Bidirectional\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import pydot_ng\n",
        "import graphviz\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "np.random.seed(5)\n",
        "\n",
        "# wandb.init(project=\"C-Mapss-project\", entity=\"raachelzhu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9jXH8hX1b6a"
      },
      "source": [
        "## Mount the dataset on a drive or specify path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "KA32fPZmaO17"
      },
      "outputs": [],
      "source": [
        "# In the NASA dataset, there are four data subsets (FD001-4). \n",
        "# This function is developed to handle any of the data subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wJvcxf1mPWDk"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "\n",
        "    \"\"\"function to turn the dataset into dataframe with column headings\"\"\"\n",
        "\n",
        "    train_df = pd.read_csv(\n",
        "        \"dataset/\" + filename + \"/train_\" + filename + \".txt\", sep=\" \", header=None\n",
        "    )\n",
        "    train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "    train_df.columns = [\n",
        "        \"id\",\n",
        "        \"cycle\",\n",
        "        \"setting1\",\n",
        "        \"setting2\",\n",
        "        \"setting3\",\n",
        "        \"s1\",\n",
        "        \"s2\",\n",
        "        \"s3\",\n",
        "        \"s4\",\n",
        "        \"s5\",\n",
        "        \"s6\",\n",
        "        \"s7\",\n",
        "        \"s8\",\n",
        "        \"s9\",\n",
        "        \"s10\",\n",
        "        \"s11\",\n",
        "        \"s12\",\n",
        "        \"s13\",\n",
        "        \"s14\",\n",
        "        \"s15\",\n",
        "        \"s16\",\n",
        "        \"s17\",\n",
        "        \"s18\",\n",
        "        \"s19\",\n",
        "        \"s20\",\n",
        "        \"s21\",\n",
        "    ]\n",
        "\n",
        "    train_df = train_df.sort_values([\"id\", \"cycle\"])\n",
        "\n",
        "    test_df = pd.read_csv(\n",
        "        \"dataset/\" + filename + \"/test_\" + filename + \".txt\", sep=\" \", header=None\n",
        "    )\n",
        "    test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "    test_df.columns = [\n",
        "        \"id\",\n",
        "        \"cycle\",\n",
        "        \"setting1\",\n",
        "        \"setting2\",\n",
        "        \"setting3\",\n",
        "        \"s1\",\n",
        "        \"s2\",\n",
        "        \"s3\",\n",
        "        \"s4\",\n",
        "        \"s5\",\n",
        "        \"s6\",\n",
        "        \"s7\",\n",
        "        \"s8\",\n",
        "        \"s9\",\n",
        "        \"s10\",\n",
        "        \"s11\",\n",
        "        \"s12\",\n",
        "        \"s13\",\n",
        "        \"s14\",\n",
        "        \"s15\",\n",
        "        \"s16\",\n",
        "        \"s17\",\n",
        "        \"s18\",\n",
        "        \"s19\",\n",
        "        \"s20\",\n",
        "        \"s21\",\n",
        "    ]\n",
        "\n",
        "    truth_df = pd.read_csv(\n",
        "        \"dataset/\" + filename + \"/RUL_\" + filename + \".txt\", sep=\" \", header=None\n",
        "    )\n",
        "    truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
        "    return train_df, test_df, truth_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jxPwNjOLP966"
      },
      "outputs": [],
      "source": [
        "#Specify the data subset to use for training\n",
        "filename= \"FD001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "UzD0xLsUegp-",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Model can be trained using a set of time windows\n",
        "# ranging from 20 to 120. Specify the window length\n",
        "\n",
        "sequence_length = 90\n",
        "\n",
        "# Not all columns in the dataset is used. Select useful signals in the dataset\n",
        "\n",
        "sequence_cols = [\"s\" + str(i) for i in set(range(1, 22))] + [\n",
        "    \"setting1\",\n",
        "    \"setting2\",\n",
        "    \"setting3\",\n",
        "    \"cycle_norm\",\n",
        "]\n",
        "\n",
        "if filename == \"FD001\":\n",
        "    cycle = 130\n",
        "    sequence_cols = [\n",
        "        x\n",
        "        for x in sequence_cols\n",
        "        # if x not in [\"s1\", \"s5\", \"s6\", \"s10\", \"s16\", \"s18\", \"s19\", \"setting3\"]\n",
        "        if x not in [\"s1\",\"s2\",\"s3\",\"s4\",\"s5\", \"s6\",\"s7\",\"s8\",\"s9\",\"s11\",\"s10\",\"s12\",\"s13\",\"s14\",\"s15\",\"s16\", \"s18\", \"s19\",\"s20\",\"s21\",\"setting3\",\"setting2\"]\n",
        "    ]\n",
        "elif filename == \"FD003\":\n",
        "    cycle = 130\n",
        "    sequence_cols = [\n",
        "        x\n",
        "        for x in sequence_cols\n",
        "        if x not in [\"s1\", \"s5\", \"s16\", \"s18\", \"s19\", \"setting3\"]\n",
        "    ]\n",
        "else:\n",
        "    cycle = 130\n",
        "    sequence_cols = [\"s\" + str(i) for i in set(range(1, 22))] + [\n",
        "        \"setting1\",\n",
        "        \"setting2\",\n",
        "        \"setting3\",\n",
        "        \"cycle_norm\",\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "rEnUSIM6QAO2"
      },
      "outputs": [],
      "source": [
        "train_df, test_df, truth_df = read_data(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "4m1YCubMQIvw",
        "outputId": "516b0eef-94bc-413b-c86f-d35f47e6d658"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cycle</th>\n",
              "      <th>setting1</th>\n",
              "      <th>setting2</th>\n",
              "      <th>setting3</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>...</th>\n",
              "      <th>s12</th>\n",
              "      <th>s13</th>\n",
              "      <th>s14</th>\n",
              "      <th>s15</th>\n",
              "      <th>s16</th>\n",
              "      <th>s17</th>\n",
              "      <th>s18</th>\n",
              "      <th>s19</th>\n",
              "      <th>s20</th>\n",
              "      <th>s21</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>-0.0004</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>641.82</td>\n",
              "      <td>1589.70</td>\n",
              "      <td>1400.60</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.66</td>\n",
              "      <td>2388.02</td>\n",
              "      <td>8138.62</td>\n",
              "      <td>8.4195</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.06</td>\n",
              "      <td>23.4190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.15</td>\n",
              "      <td>1591.82</td>\n",
              "      <td>1403.14</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>522.28</td>\n",
              "      <td>2388.07</td>\n",
              "      <td>8131.49</td>\n",
              "      <td>8.4318</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.00</td>\n",
              "      <td>23.4236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.0043</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.35</td>\n",
              "      <td>1587.99</td>\n",
              "      <td>1404.20</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>522.42</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8133.23</td>\n",
              "      <td>8.4178</td>\n",
              "      <td>0.03</td>\n",
              "      <td>390</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.95</td>\n",
              "      <td>23.3442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.35</td>\n",
              "      <td>1582.79</td>\n",
              "      <td>1401.87</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>522.86</td>\n",
              "      <td>2388.08</td>\n",
              "      <td>8133.83</td>\n",
              "      <td>8.3682</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.88</td>\n",
              "      <td>23.3739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.0019</td>\n",
              "      <td>-0.0002</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.37</td>\n",
              "      <td>1582.85</td>\n",
              "      <td>1406.22</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>522.19</td>\n",
              "      <td>2388.04</td>\n",
              "      <td>8133.80</td>\n",
              "      <td>8.4294</td>\n",
              "      <td>0.03</td>\n",
              "      <td>393</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.90</td>\n",
              "      <td>23.4044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
              "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
              "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
              "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
              "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
              "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
              "\n",
              "      s5  ...     s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
              "0  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388  100.0   \n",
              "1  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388  100.0   \n",
              "2  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388  100.0   \n",
              "3  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388  100.0   \n",
              "4  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388  100.0   \n",
              "\n",
              "     s20      s21  \n",
              "0  39.06  23.4190  \n",
              "1  39.00  23.4236  \n",
              "2  38.95  23.3442  \n",
              "3  38.88  23.3739  \n",
              "4  38.90  23.4044  \n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "ucQdHHzKQMCk",
        "outputId": "17ed9019-74cc-49c7-ad7e-51ebf4505e1d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cycle</th>\n",
              "      <th>setting1</th>\n",
              "      <th>setting2</th>\n",
              "      <th>setting3</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>...</th>\n",
              "      <th>s12</th>\n",
              "      <th>s13</th>\n",
              "      <th>s14</th>\n",
              "      <th>s15</th>\n",
              "      <th>s16</th>\n",
              "      <th>s17</th>\n",
              "      <th>s18</th>\n",
              "      <th>s19</th>\n",
              "      <th>s20</th>\n",
              "      <th>s21</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.02</td>\n",
              "      <td>1585.29</td>\n",
              "      <td>1398.21</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.72</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8125.55</td>\n",
              "      <td>8.4052</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.86</td>\n",
              "      <td>23.3735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.0027</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>641.71</td>\n",
              "      <td>1588.45</td>\n",
              "      <td>1395.42</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>522.16</td>\n",
              "      <td>2388.06</td>\n",
              "      <td>8139.62</td>\n",
              "      <td>8.3803</td>\n",
              "      <td>0.03</td>\n",
              "      <td>393</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.02</td>\n",
              "      <td>23.3916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.46</td>\n",
              "      <td>1586.94</td>\n",
              "      <td>1401.34</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.97</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8130.10</td>\n",
              "      <td>8.4441</td>\n",
              "      <td>0.03</td>\n",
              "      <td>393</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.08</td>\n",
              "      <td>23.4166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.44</td>\n",
              "      <td>1584.12</td>\n",
              "      <td>1406.42</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.38</td>\n",
              "      <td>2388.05</td>\n",
              "      <td>8132.90</td>\n",
              "      <td>8.3917</td>\n",
              "      <td>0.03</td>\n",
              "      <td>391</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.00</td>\n",
              "      <td>23.3737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.51</td>\n",
              "      <td>1587.19</td>\n",
              "      <td>1401.92</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>522.15</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8129.54</td>\n",
              "      <td>8.4031</td>\n",
              "      <td>0.03</td>\n",
              "      <td>390</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.99</td>\n",
              "      <td>23.4130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13091</th>\n",
              "      <td>100</td>\n",
              "      <td>194</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.24</td>\n",
              "      <td>1599.45</td>\n",
              "      <td>1415.79</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>520.69</td>\n",
              "      <td>2388.00</td>\n",
              "      <td>8213.28</td>\n",
              "      <td>8.4715</td>\n",
              "      <td>0.03</td>\n",
              "      <td>394</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.65</td>\n",
              "      <td>23.1974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13092</th>\n",
              "      <td>100</td>\n",
              "      <td>195</td>\n",
              "      <td>-0.0011</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.22</td>\n",
              "      <td>1595.69</td>\n",
              "      <td>1422.05</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.05</td>\n",
              "      <td>2388.09</td>\n",
              "      <td>8210.85</td>\n",
              "      <td>8.4512</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.57</td>\n",
              "      <td>23.2771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13093</th>\n",
              "      <td>100</td>\n",
              "      <td>196</td>\n",
              "      <td>-0.0006</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.44</td>\n",
              "      <td>1593.15</td>\n",
              "      <td>1406.82</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.18</td>\n",
              "      <td>2388.04</td>\n",
              "      <td>8217.24</td>\n",
              "      <td>8.4569</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.62</td>\n",
              "      <td>23.2051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13094</th>\n",
              "      <td>100</td>\n",
              "      <td>197</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.26</td>\n",
              "      <td>1594.99</td>\n",
              "      <td>1419.36</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.33</td>\n",
              "      <td>2388.08</td>\n",
              "      <td>8220.48</td>\n",
              "      <td>8.4711</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.66</td>\n",
              "      <td>23.2699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13095</th>\n",
              "      <td>100</td>\n",
              "      <td>198</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.95</td>\n",
              "      <td>1601.62</td>\n",
              "      <td>1424.99</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>521.07</td>\n",
              "      <td>2388.05</td>\n",
              "      <td>8214.64</td>\n",
              "      <td>8.4903</td>\n",
              "      <td>0.03</td>\n",
              "      <td>396</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.70</td>\n",
              "      <td>23.1855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13096 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
              "0        1      1    0.0023    0.0003     100.0  518.67  643.02  1585.29   \n",
              "1        1      2   -0.0027   -0.0003     100.0  518.67  641.71  1588.45   \n",
              "2        1      3    0.0003    0.0001     100.0  518.67  642.46  1586.94   \n",
              "3        1      4    0.0042    0.0000     100.0  518.67  642.44  1584.12   \n",
              "4        1      5    0.0014    0.0000     100.0  518.67  642.51  1587.19   \n",
              "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
              "13091  100    194    0.0049    0.0000     100.0  518.67  643.24  1599.45   \n",
              "13092  100    195   -0.0011   -0.0001     100.0  518.67  643.22  1595.69   \n",
              "13093  100    196   -0.0006   -0.0003     100.0  518.67  643.44  1593.15   \n",
              "13094  100    197   -0.0038    0.0001     100.0  518.67  643.26  1594.99   \n",
              "13095  100    198    0.0013    0.0003     100.0  518.67  642.95  1601.62   \n",
              "\n",
              "            s4     s5  ...     s12      s13      s14     s15   s16  s17   s18  \\\n",
              "0      1398.21  14.62  ...  521.72  2388.03  8125.55  8.4052  0.03  392  2388   \n",
              "1      1395.42  14.62  ...  522.16  2388.06  8139.62  8.3803  0.03  393  2388   \n",
              "2      1401.34  14.62  ...  521.97  2388.03  8130.10  8.4441  0.03  393  2388   \n",
              "3      1406.42  14.62  ...  521.38  2388.05  8132.90  8.3917  0.03  391  2388   \n",
              "4      1401.92  14.62  ...  522.15  2388.03  8129.54  8.4031  0.03  390  2388   \n",
              "...        ...    ...  ...     ...      ...      ...     ...   ...  ...   ...   \n",
              "13091  1415.79  14.62  ...  520.69  2388.00  8213.28  8.4715  0.03  394  2388   \n",
              "13092  1422.05  14.62  ...  521.05  2388.09  8210.85  8.4512  0.03  395  2388   \n",
              "13093  1406.82  14.62  ...  521.18  2388.04  8217.24  8.4569  0.03  395  2388   \n",
              "13094  1419.36  14.62  ...  521.33  2388.08  8220.48  8.4711  0.03  395  2388   \n",
              "13095  1424.99  14.62  ...  521.07  2388.05  8214.64  8.4903  0.03  396  2388   \n",
              "\n",
              "         s19    s20      s21  \n",
              "0      100.0  38.86  23.3735  \n",
              "1      100.0  39.02  23.3916  \n",
              "2      100.0  39.08  23.4166  \n",
              "3      100.0  39.00  23.3737  \n",
              "4      100.0  38.99  23.4130  \n",
              "...      ...    ...      ...  \n",
              "13091  100.0  38.65  23.1974  \n",
              "13092  100.0  38.57  23.2771  \n",
              "13093  100.0  38.62  23.2051  \n",
              "13094  100.0  38.66  23.2699  \n",
              "13095  100.0  38.70  23.1855  \n",
              "\n",
              "[13096 rows x 26 columns]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "mlRIehUzQRmY"
      },
      "outputs": [],
      "source": [
        "#Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-YmVjz9RMd2U"
      },
      "outputs": [],
      "source": [
        "# These two functions preprocess the training and test dataset respectively\n",
        "\n",
        "def train_data_prep(train_df):\n",
        "      \"\"\"\n",
        "       A function to preprocess the training dataset for piece-wise RUL\n",
        "       estimation\n",
        "       \n",
        "       Parameter:\n",
        "         \n",
        "         training dataframe (FD001-4)\n",
        "        \n",
        "       Returns:\n",
        "         \n",
        "         train_df, a preprocessed dataframe with column names\n",
        "        and target variable.\n",
        "\n",
        "      \"\"\"\n",
        "      rul = pd.DataFrame(train_df.groupby(\"id\")[\"cycle\"].max()).reset_index()\n",
        "      rul.columns = [\"id\", \"max\"]\n",
        "      train_df = train_df.merge(rul, on=[\"id\"], how=\"left\")\n",
        "      train_df[\"RUL\"] = train_df[\"max\"] - train_df[\"cycle\"]\n",
        "      train_df.drop(\"max\", axis=1, inplace=True)\n",
        "      train_df[\"R_early\"] = train_df[\"RUL\"].apply(lambda x: cycle if x >= cycle else x)\n",
        "      train_df = train_df.drop([\"RUL\"], axis=1)\n",
        "      return train_df\n",
        "\n",
        "def test_data_prep(test_df, truth_df):\n",
        "    \"\"\"\n",
        "    A function to preprocess the test data subset for\n",
        "     piece-wise RUL prediction\n",
        "          \n",
        "        Parameters:\n",
        "\n",
        "          truth_df: the actual RUL\n",
        "          test_df: the test data subset\n",
        "\n",
        "        Returns:\n",
        "\n",
        "          test_df: a preprocessed test dataset with piecewise RUL.\n",
        "          \"\"\"\n",
        "    rul = pd.DataFrame(test_df.groupby(\"id\")[\"cycle\"].max()).reset_index()\n",
        "    rul.columns = [\"id\", \"max\"]\n",
        "    truth_df.columns = [\"more\"]\n",
        "    truth_df[\"id\"] = truth_df.index + 1\n",
        "    truth_df[\"max\"] = rul[\"max\"] + truth_df[\"more\"]\n",
        "    truth_df.drop(\"more\", axis=1, inplace=True)\n",
        "    # generate RUL for test data\n",
        "    test_df = test_df.merge(truth_df, on=[\"id\"], how=\"left\")\n",
        "    test_df[\"RUL\"] = test_df[\"max\"] - test_df[\"cycle\"]\n",
        "    test_df.drop(\"max\", axis=1, inplace=True)\n",
        "    test_df[\"R_early\"] = test_df[\"RUL\"].apply(lambda x: cycle if x >= cycle else x)\n",
        "    test_df = test_df.drop([\"RUL\"], axis=1)\n",
        "    return test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JBDefzFqQf0I"
      },
      "outputs": [],
      "source": [
        "train_df = train_data_prep(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "A_anRFQVQsjc",
        "outputId": "90dbfc24-59e7-46f8-f50d-6c23687ef03c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cycle</th>\n",
              "      <th>setting1</th>\n",
              "      <th>setting2</th>\n",
              "      <th>setting3</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>...</th>\n",
              "      <th>s13</th>\n",
              "      <th>s14</th>\n",
              "      <th>s15</th>\n",
              "      <th>s16</th>\n",
              "      <th>s17</th>\n",
              "      <th>s18</th>\n",
              "      <th>s19</th>\n",
              "      <th>s20</th>\n",
              "      <th>s21</th>\n",
              "      <th>R_early</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>-0.0004</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>641.82</td>\n",
              "      <td>1589.70</td>\n",
              "      <td>1400.60</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.02</td>\n",
              "      <td>8138.62</td>\n",
              "      <td>8.4195</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.06</td>\n",
              "      <td>23.4190</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.15</td>\n",
              "      <td>1591.82</td>\n",
              "      <td>1403.14</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.07</td>\n",
              "      <td>8131.49</td>\n",
              "      <td>8.4318</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.00</td>\n",
              "      <td>23.4236</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.0043</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.35</td>\n",
              "      <td>1587.99</td>\n",
              "      <td>1404.20</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8133.23</td>\n",
              "      <td>8.4178</td>\n",
              "      <td>0.03</td>\n",
              "      <td>390</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.95</td>\n",
              "      <td>23.3442</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.35</td>\n",
              "      <td>1582.79</td>\n",
              "      <td>1401.87</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.08</td>\n",
              "      <td>8133.83</td>\n",
              "      <td>8.3682</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.88</td>\n",
              "      <td>23.3739</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.0019</td>\n",
              "      <td>-0.0002</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.37</td>\n",
              "      <td>1582.85</td>\n",
              "      <td>1406.22</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.04</td>\n",
              "      <td>8133.80</td>\n",
              "      <td>8.4294</td>\n",
              "      <td>0.03</td>\n",
              "      <td>393</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.90</td>\n",
              "      <td>23.4044</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20626</th>\n",
              "      <td>100</td>\n",
              "      <td>196</td>\n",
              "      <td>-0.0004</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.49</td>\n",
              "      <td>1597.98</td>\n",
              "      <td>1428.63</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.26</td>\n",
              "      <td>8137.60</td>\n",
              "      <td>8.4956</td>\n",
              "      <td>0.03</td>\n",
              "      <td>397</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.49</td>\n",
              "      <td>22.9735</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20627</th>\n",
              "      <td>100</td>\n",
              "      <td>197</td>\n",
              "      <td>-0.0016</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.54</td>\n",
              "      <td>1604.50</td>\n",
              "      <td>1433.58</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.22</td>\n",
              "      <td>8136.50</td>\n",
              "      <td>8.5139</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.30</td>\n",
              "      <td>23.1594</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20628</th>\n",
              "      <td>100</td>\n",
              "      <td>198</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.42</td>\n",
              "      <td>1602.46</td>\n",
              "      <td>1428.18</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.24</td>\n",
              "      <td>8141.05</td>\n",
              "      <td>8.5646</td>\n",
              "      <td>0.03</td>\n",
              "      <td>398</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.44</td>\n",
              "      <td>22.9333</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20629</th>\n",
              "      <td>100</td>\n",
              "      <td>199</td>\n",
              "      <td>-0.0011</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.23</td>\n",
              "      <td>1605.26</td>\n",
              "      <td>1426.53</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.23</td>\n",
              "      <td>8139.29</td>\n",
              "      <td>8.5389</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.29</td>\n",
              "      <td>23.0640</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20630</th>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>-0.0032</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.85</td>\n",
              "      <td>1600.38</td>\n",
              "      <td>1432.14</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.26</td>\n",
              "      <td>8137.33</td>\n",
              "      <td>8.5036</td>\n",
              "      <td>0.03</td>\n",
              "      <td>396</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.37</td>\n",
              "      <td>23.0522</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20631 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
              "0        1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
              "1        1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
              "2        1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
              "3        1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
              "4        1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
              "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
              "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
              "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
              "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
              "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
              "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
              "\n",
              "            s4     s5  ...      s13      s14     s15   s16  s17   s18    s19  \\\n",
              "0      1400.60  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100.0   \n",
              "1      1403.14  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100.0   \n",
              "2      1404.20  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100.0   \n",
              "3      1401.87  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100.0   \n",
              "4      1406.22  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100.0   \n",
              "...        ...    ...  ...      ...      ...     ...   ...  ...   ...    ...   \n",
              "20626  1428.63  14.62  ...  2388.26  8137.60  8.4956  0.03  397  2388  100.0   \n",
              "20627  1433.58  14.62  ...  2388.22  8136.50  8.5139  0.03  395  2388  100.0   \n",
              "20628  1428.18  14.62  ...  2388.24  8141.05  8.5646  0.03  398  2388  100.0   \n",
              "20629  1426.53  14.62  ...  2388.23  8139.29  8.5389  0.03  395  2388  100.0   \n",
              "20630  1432.14  14.62  ...  2388.26  8137.33  8.5036  0.03  396  2388  100.0   \n",
              "\n",
              "         s20      s21  R_early  \n",
              "0      39.06  23.4190      130  \n",
              "1      39.00  23.4236      130  \n",
              "2      38.95  23.3442      130  \n",
              "3      38.88  23.3739      130  \n",
              "4      38.90  23.4044      130  \n",
              "...      ...      ...      ...  \n",
              "20626  38.49  22.9735        4  \n",
              "20627  38.30  23.1594        3  \n",
              "20628  38.44  22.9333        2  \n",
              "20629  38.29  23.0640        1  \n",
              "20630  38.37  23.0522        0  \n",
              "\n",
              "[20631 rows x 27 columns]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "gjQ3QmxrQ2f0"
      },
      "outputs": [],
      "source": [
        "test_df= test_data_prep(test_df, truth_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "ku5Qh9uZRwes",
        "outputId": "cf0ac4ac-2a37-4456-b3b5-7681ab3044e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cycle</th>\n",
              "      <th>setting1</th>\n",
              "      <th>setting2</th>\n",
              "      <th>setting3</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>...</th>\n",
              "      <th>s13</th>\n",
              "      <th>s14</th>\n",
              "      <th>s15</th>\n",
              "      <th>s16</th>\n",
              "      <th>s17</th>\n",
              "      <th>s18</th>\n",
              "      <th>s19</th>\n",
              "      <th>s20</th>\n",
              "      <th>s21</th>\n",
              "      <th>R_early</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.02</td>\n",
              "      <td>1585.29</td>\n",
              "      <td>1398.21</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8125.55</td>\n",
              "      <td>8.4052</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.86</td>\n",
              "      <td>23.3735</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.0027</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>641.71</td>\n",
              "      <td>1588.45</td>\n",
              "      <td>1395.42</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.06</td>\n",
              "      <td>8139.62</td>\n",
              "      <td>8.3803</td>\n",
              "      <td>0.03</td>\n",
              "      <td>393</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.02</td>\n",
              "      <td>23.3916</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.46</td>\n",
              "      <td>1586.94</td>\n",
              "      <td>1401.34</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8130.10</td>\n",
              "      <td>8.4441</td>\n",
              "      <td>0.03</td>\n",
              "      <td>393</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.08</td>\n",
              "      <td>23.4166</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.44</td>\n",
              "      <td>1584.12</td>\n",
              "      <td>1406.42</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.05</td>\n",
              "      <td>8132.90</td>\n",
              "      <td>8.3917</td>\n",
              "      <td>0.03</td>\n",
              "      <td>391</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>39.00</td>\n",
              "      <td>23.3737</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.51</td>\n",
              "      <td>1587.19</td>\n",
              "      <td>1401.92</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.03</td>\n",
              "      <td>8129.54</td>\n",
              "      <td>8.4031</td>\n",
              "      <td>0.03</td>\n",
              "      <td>390</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.99</td>\n",
              "      <td>23.4130</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13091</th>\n",
              "      <td>100</td>\n",
              "      <td>194</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.24</td>\n",
              "      <td>1599.45</td>\n",
              "      <td>1415.79</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.00</td>\n",
              "      <td>8213.28</td>\n",
              "      <td>8.4715</td>\n",
              "      <td>0.03</td>\n",
              "      <td>394</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.65</td>\n",
              "      <td>23.1974</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13092</th>\n",
              "      <td>100</td>\n",
              "      <td>195</td>\n",
              "      <td>-0.0011</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.22</td>\n",
              "      <td>1595.69</td>\n",
              "      <td>1422.05</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.09</td>\n",
              "      <td>8210.85</td>\n",
              "      <td>8.4512</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.57</td>\n",
              "      <td>23.2771</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13093</th>\n",
              "      <td>100</td>\n",
              "      <td>196</td>\n",
              "      <td>-0.0006</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.44</td>\n",
              "      <td>1593.15</td>\n",
              "      <td>1406.82</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.04</td>\n",
              "      <td>8217.24</td>\n",
              "      <td>8.4569</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.62</td>\n",
              "      <td>23.2051</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13094</th>\n",
              "      <td>100</td>\n",
              "      <td>197</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.26</td>\n",
              "      <td>1594.99</td>\n",
              "      <td>1419.36</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.08</td>\n",
              "      <td>8220.48</td>\n",
              "      <td>8.4711</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.66</td>\n",
              "      <td>23.2699</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13095</th>\n",
              "      <td>100</td>\n",
              "      <td>198</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>642.95</td>\n",
              "      <td>1601.62</td>\n",
              "      <td>1424.99</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2388.05</td>\n",
              "      <td>8214.64</td>\n",
              "      <td>8.4903</td>\n",
              "      <td>0.03</td>\n",
              "      <td>396</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>38.70</td>\n",
              "      <td>23.1855</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13096 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
              "0        1      1    0.0023    0.0003     100.0  518.67  643.02  1585.29   \n",
              "1        1      2   -0.0027   -0.0003     100.0  518.67  641.71  1588.45   \n",
              "2        1      3    0.0003    0.0001     100.0  518.67  642.46  1586.94   \n",
              "3        1      4    0.0042    0.0000     100.0  518.67  642.44  1584.12   \n",
              "4        1      5    0.0014    0.0000     100.0  518.67  642.51  1587.19   \n",
              "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
              "13091  100    194    0.0049    0.0000     100.0  518.67  643.24  1599.45   \n",
              "13092  100    195   -0.0011   -0.0001     100.0  518.67  643.22  1595.69   \n",
              "13093  100    196   -0.0006   -0.0003     100.0  518.67  643.44  1593.15   \n",
              "13094  100    197   -0.0038    0.0001     100.0  518.67  643.26  1594.99   \n",
              "13095  100    198    0.0013    0.0003     100.0  518.67  642.95  1601.62   \n",
              "\n",
              "            s4     s5  ...      s13      s14     s15   s16  s17   s18    s19  \\\n",
              "0      1398.21  14.62  ...  2388.03  8125.55  8.4052  0.03  392  2388  100.0   \n",
              "1      1395.42  14.62  ...  2388.06  8139.62  8.3803  0.03  393  2388  100.0   \n",
              "2      1401.34  14.62  ...  2388.03  8130.10  8.4441  0.03  393  2388  100.0   \n",
              "3      1406.42  14.62  ...  2388.05  8132.90  8.3917  0.03  391  2388  100.0   \n",
              "4      1401.92  14.62  ...  2388.03  8129.54  8.4031  0.03  390  2388  100.0   \n",
              "...        ...    ...  ...      ...      ...     ...   ...  ...   ...    ...   \n",
              "13091  1415.79  14.62  ...  2388.00  8213.28  8.4715  0.03  394  2388  100.0   \n",
              "13092  1422.05  14.62  ...  2388.09  8210.85  8.4512  0.03  395  2388  100.0   \n",
              "13093  1406.82  14.62  ...  2388.04  8217.24  8.4569  0.03  395  2388  100.0   \n",
              "13094  1419.36  14.62  ...  2388.08  8220.48  8.4711  0.03  395  2388  100.0   \n",
              "13095  1424.99  14.62  ...  2388.05  8214.64  8.4903  0.03  396  2388  100.0   \n",
              "\n",
              "         s20      s21  R_early  \n",
              "0      38.86  23.3735      130  \n",
              "1      39.02  23.3916      130  \n",
              "2      39.08  23.4166      130  \n",
              "3      39.00  23.3737      130  \n",
              "4      38.99  23.4130      130  \n",
              "...      ...      ...      ...  \n",
              "13091  38.65  23.1974       24  \n",
              "13092  38.57  23.2771       23  \n",
              "13093  38.62  23.2051       22  \n",
              "13094  38.66  23.2699       21  \n",
              "13095  38.70  23.1855       20  \n",
              "\n",
              "[13096 rows x 27 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "-N5LkJ36SAWM"
      },
      "outputs": [],
      "source": [
        "#Normalize the dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Q3SIauqCNBBI"
      },
      "outputs": [],
      "source": [
        "#This function normalizes the training and test dataset.\n",
        "def train_test_scaler(train_data, test_data):\n",
        "\n",
        "    \"\"\"function to standardize the dataset\"\"\"\n",
        "\n",
        "    train_data[\"cycle_norm\"] = train_data[\"cycle\"]\n",
        "    cols_normalize = train_data.columns.difference([\"id\", \"cycle\", \"R_early\"])\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    norm_train_df = pd.DataFrame(\n",
        "        min_max_scaler.fit_transform(train_data[cols_normalize]),\n",
        "        columns=cols_normalize,\n",
        "        index=train_data.index,\n",
        "    )\n",
        "    join_df = train_data[train_data.columns.difference(cols_normalize)].join(\n",
        "        norm_train_df\n",
        "    )\n",
        "    train_df = join_df.reindex(columns=train_data.columns)\n",
        "\n",
        "    # MinMax normalization (from 0 to 1)\n",
        "    test_data[\"cycle_norm\"] = test_data[\"cycle\"]\n",
        "    norm_test_df = pd.DataFrame(\n",
        "        min_max_scaler.transform(test_data[cols_normalize]),\n",
        "        columns=cols_normalize,\n",
        "        index=test_data.index,\n",
        "    )\n",
        "    test_join_df = test_data[test_data.columns.difference(cols_normalize)].join(\n",
        "        norm_test_df\n",
        "    )\n",
        "    test_df = test_join_df.reindex(columns=test_data.columns)\n",
        "    test_df = test_df.reset_index(drop=True)\n",
        "    return train_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "AFNSzEIG7gG8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cycle</th>\n",
              "      <th>setting1</th>\n",
              "      <th>setting2</th>\n",
              "      <th>setting3</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>...</th>\n",
              "      <th>s14</th>\n",
              "      <th>s15</th>\n",
              "      <th>s16</th>\n",
              "      <th>s17</th>\n",
              "      <th>s18</th>\n",
              "      <th>s19</th>\n",
              "      <th>s20</th>\n",
              "      <th>s21</th>\n",
              "      <th>R_early</th>\n",
              "      <th>cycle_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.459770</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.183735</td>\n",
              "      <td>0.406802</td>\n",
              "      <td>0.309757</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.199608</td>\n",
              "      <td>0.363986</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.713178</td>\n",
              "      <td>0.724662</td>\n",
              "      <td>130</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.609195</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.283133</td>\n",
              "      <td>0.453019</td>\n",
              "      <td>0.352633</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.162813</td>\n",
              "      <td>0.411312</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.731014</td>\n",
              "      <td>130</td>\n",
              "      <td>0.002770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.252874</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.343373</td>\n",
              "      <td>0.369523</td>\n",
              "      <td>0.370527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.171793</td>\n",
              "      <td>0.357445</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.627907</td>\n",
              "      <td>0.621375</td>\n",
              "      <td>130</td>\n",
              "      <td>0.005540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.540230</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.343373</td>\n",
              "      <td>0.256159</td>\n",
              "      <td>0.331195</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.174889</td>\n",
              "      <td>0.166603</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573643</td>\n",
              "      <td>0.662386</td>\n",
              "      <td>130</td>\n",
              "      <td>0.008310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.390805</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.349398</td>\n",
              "      <td>0.257467</td>\n",
              "      <td>0.404625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.174734</td>\n",
              "      <td>0.402078</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.589147</td>\n",
              "      <td>0.704502</td>\n",
              "      <td>130</td>\n",
              "      <td>0.011080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20626</th>\n",
              "      <td>100</td>\n",
              "      <td>196</td>\n",
              "      <td>0.477011</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.686747</td>\n",
              "      <td>0.587312</td>\n",
              "      <td>0.782917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.194344</td>\n",
              "      <td>0.656791</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.271318</td>\n",
              "      <td>0.109500</td>\n",
              "      <td>4</td>\n",
              "      <td>0.540166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20627</th>\n",
              "      <td>100</td>\n",
              "      <td>197</td>\n",
              "      <td>0.408046</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.701807</td>\n",
              "      <td>0.729453</td>\n",
              "      <td>0.866475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188668</td>\n",
              "      <td>0.727203</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.124031</td>\n",
              "      <td>0.366197</td>\n",
              "      <td>3</td>\n",
              "      <td>0.542936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20628</th>\n",
              "      <td>100</td>\n",
              "      <td>198</td>\n",
              "      <td>0.522989</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.665663</td>\n",
              "      <td>0.684979</td>\n",
              "      <td>0.775321</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.212148</td>\n",
              "      <td>0.922278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.232558</td>\n",
              "      <td>0.053991</td>\n",
              "      <td>2</td>\n",
              "      <td>0.545706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20629</th>\n",
              "      <td>100</td>\n",
              "      <td>199</td>\n",
              "      <td>0.436782</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.608434</td>\n",
              "      <td>0.746021</td>\n",
              "      <td>0.747468</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.203065</td>\n",
              "      <td>0.823394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.116279</td>\n",
              "      <td>0.234466</td>\n",
              "      <td>1</td>\n",
              "      <td>0.548476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20630</th>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.795181</td>\n",
              "      <td>0.639634</td>\n",
              "      <td>0.842167</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.192951</td>\n",
              "      <td>0.687572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.178295</td>\n",
              "      <td>0.218172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.551247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20631 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
              "0        1      1  0.459770  0.166667       0.0  0.0  0.183735  0.406802   \n",
              "1        1      2  0.609195  0.250000       0.0  0.0  0.283133  0.453019   \n",
              "2        1      3  0.252874  0.750000       0.0  0.0  0.343373  0.369523   \n",
              "3        1      4  0.540230  0.500000       0.0  0.0  0.343373  0.256159   \n",
              "4        1      5  0.390805  0.333333       0.0  0.0  0.349398  0.257467   \n",
              "...    ...    ...       ...       ...       ...  ...       ...       ...   \n",
              "20626  100    196  0.477011  0.250000       0.0  0.0  0.686747  0.587312   \n",
              "20627  100    197  0.408046  0.083333       0.0  0.0  0.701807  0.729453   \n",
              "20628  100    198  0.522989  0.500000       0.0  0.0  0.665663  0.684979   \n",
              "20629  100    199  0.436782  0.750000       0.0  0.0  0.608434  0.746021   \n",
              "20630  100    200  0.316092  0.083333       0.0  0.0  0.795181  0.639634   \n",
              "\n",
              "             s4   s5  ...       s14       s15  s16       s17  s18  s19  \\\n",
              "0      0.309757  0.0  ...  0.199608  0.363986  0.0  0.333333  0.0  0.0   \n",
              "1      0.352633  0.0  ...  0.162813  0.411312  0.0  0.333333  0.0  0.0   \n",
              "2      0.370527  0.0  ...  0.171793  0.357445  0.0  0.166667  0.0  0.0   \n",
              "3      0.331195  0.0  ...  0.174889  0.166603  0.0  0.333333  0.0  0.0   \n",
              "4      0.404625  0.0  ...  0.174734  0.402078  0.0  0.416667  0.0  0.0   \n",
              "...         ...  ...  ...       ...       ...  ...       ...  ...  ...   \n",
              "20626  0.782917  0.0  ...  0.194344  0.656791  0.0  0.750000  0.0  0.0   \n",
              "20627  0.866475  0.0  ...  0.188668  0.727203  0.0  0.583333  0.0  0.0   \n",
              "20628  0.775321  0.0  ...  0.212148  0.922278  0.0  0.833333  0.0  0.0   \n",
              "20629  0.747468  0.0  ...  0.203065  0.823394  0.0  0.583333  0.0  0.0   \n",
              "20630  0.842167  0.0  ...  0.192951  0.687572  0.0  0.666667  0.0  0.0   \n",
              "\n",
              "            s20       s21  R_early  cycle_norm  \n",
              "0      0.713178  0.724662      130    0.000000  \n",
              "1      0.666667  0.731014      130    0.002770  \n",
              "2      0.627907  0.621375      130    0.005540  \n",
              "3      0.573643  0.662386      130    0.008310  \n",
              "4      0.589147  0.704502      130    0.011080  \n",
              "...         ...       ...      ...         ...  \n",
              "20626  0.271318  0.109500        4    0.540166  \n",
              "20627  0.124031  0.366197        3    0.542936  \n",
              "20628  0.232558  0.053991        2    0.545706  \n",
              "20629  0.116279  0.234466        1    0.548476  \n",
              "20630  0.178295  0.218172        0    0.551247  \n",
              "\n",
              "[20631 rows x 28 columns]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df, test_df = train_test_scaler(train_df, test_df)\n",
        "\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "4R4ZsoDFNbpa"
      },
      "outputs": [],
      "source": [
        "#These functions reshape the training and test datasets in three dimension\n",
        "\n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "\n",
        "    \"\"\"\n",
        "    sequence generator\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "        id_df: Engine ID\n",
        "        seq_length: Sequence length\n",
        "        seq_cols: Sequence column\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "\n",
        "    for start, stop in zip(\n",
        "        range(0, num_elements - seq_length), range(seq_length, num_elements)\n",
        "    ):\n",
        "        yield data_matrix[start:stop, :]\n",
        "\n",
        "\n",
        "def sequence_gen(train_data, sequence_length, sequence_cols):\n",
        "\n",
        "    \"\"\"\n",
        "    training sequence generator.\n",
        "\n",
        "    Returns:\n",
        "        Generated sequence in the form\n",
        "        [samples, time_steps, features]\n",
        "\n",
        "    \"\"\"\n",
        "    seq_gen = (\n",
        "        list(\n",
        "            gen_sequence(\n",
        "                train_data[train_data[\"id\"] == id], sequence_length, sequence_cols\n",
        "            )\n",
        "        )\n",
        "        for id in train_data[\"id\"].unique()\n",
        "    )\n",
        "    # generate sequences and convert to numpy array\n",
        "    seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "    return seq_array\n",
        "\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "\n",
        "def label_generator(train_data, sequence_length, sequence_cols):\n",
        "    \"\"\" generate labels \"\"\"\n",
        "    label_gen = [\n",
        "        gen_labels(train_data[train_data[\"id\"] == id], sequence_length, [\"R_early\"])\n",
        "        for id in train_data[\"id\"].unique()\n",
        "    ]\n",
        "    label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "    return label_array\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nF1YokwheMl",
        "outputId": "480cf48b-abe1-4d0e-fc07-7b72a87ad82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Shape:  (11631, 90, 3)\n",
            "Label Shape:  (11631, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[0.33333334, 0.4597701 , 0.        ],\n",
              "        [0.33333334, 0.6091954 , 0.00277008],\n",
              "        [0.16666667, 0.25287357, 0.00554017],\n",
              "        ...,\n",
              "        [0.33333334, 0.6666667 , 0.24099723],\n",
              "        [0.33333334, 0.545977  , 0.2437673 ],\n",
              "        [0.25      , 0.59770113, 0.2465374 ]],\n",
              "\n",
              "       [[0.33333334, 0.6091954 , 0.00277008],\n",
              "        [0.16666667, 0.25287357, 0.00554017],\n",
              "        [0.33333334, 0.54022986, 0.00831025],\n",
              "        ...,\n",
              "        [0.33333334, 0.545977  , 0.2437673 ],\n",
              "        [0.25      , 0.59770113, 0.2465374 ],\n",
              "        [0.5       , 0.6436782 , 0.24930748]],\n",
              "\n",
              "       [[0.16666667, 0.25287357, 0.00554017],\n",
              "        [0.33333334, 0.54022986, 0.00831025],\n",
              "        [0.41666666, 0.3908046 , 0.01108033],\n",
              "        ...,\n",
              "        [0.25      , 0.59770113, 0.2465374 ],\n",
              "        [0.5       , 0.6436782 , 0.24930748],\n",
              "        [0.33333334, 0.5574713 , 0.25207755]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.5       , 0.21839081, 0.29639888],\n",
              "        [0.5       , 0.6666667 , 0.29916897],\n",
              "        [0.5       , 0.5344828 , 0.30193907],\n",
              "        ...,\n",
              "        [0.5       , 0.48850575, 0.53739613],\n",
              "        [0.75      , 0.4770115 , 0.5401662 ],\n",
              "        [0.5833333 , 0.40804598, 0.54293627]],\n",
              "\n",
              "       [[0.5       , 0.6666667 , 0.29916897],\n",
              "        [0.5       , 0.5344828 , 0.30193907],\n",
              "        [0.5       , 0.3908046 , 0.30470914],\n",
              "        ...,\n",
              "        [0.75      , 0.4770115 , 0.5401662 ],\n",
              "        [0.5833333 , 0.40804598, 0.54293627],\n",
              "        [0.8333333 , 0.5229885 , 0.5457064 ]],\n",
              "\n",
              "       [[0.5       , 0.5344828 , 0.30193907],\n",
              "        [0.5       , 0.3908046 , 0.30470914],\n",
              "        [0.33333334, 0.33333334, 0.30747923],\n",
              "        ...,\n",
              "        [0.5833333 , 0.40804598, 0.54293627],\n",
              "        [0.8333333 , 0.5229885 , 0.5457064 ],\n",
              "        [0.5833333 , 0.43678162, 0.54847646]]], dtype=float32)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Specify training sample, training label\n",
        "seq_array = sequence_gen(train_df, sequence_length, sequence_cols)\n",
        "label_array = label_generator(train_df, sequence_length, ['R_early'])\n",
        "print('Data Shape: ', seq_array.shape)\n",
        "print('Label Shape: ', label_array.shape)\n",
        "seq_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "YsNE6RNEX6u4"
      },
      "outputs": [],
      "source": [
        "# Similarly, prepare the test data for model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Ks_2qn2mXzfk"
      },
      "outputs": [],
      "source": [
        "# prepare test data for model evaluation\n",
        "def test_data_eval(test_df, sequence_length, sequence_cols):\n",
        "\n",
        "    \"\"\"\n",
        "    This function pick the last sequence for each\n",
        "    engine in the test data for model evaluation\n",
        "\n",
        "    \"\"\"\n",
        "    seq_array_test_last = [\n",
        "        test_df[test_df[\"id\"] == id][sequence_cols].values[-sequence_length:]\n",
        "        for id in test_df[\"id\"].unique()\n",
        "        if len(test_df[test_df[\"id\"] == id]) >= sequence_length\n",
        "    ]\n",
        "    seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
        "\n",
        "    y_mask = [\n",
        "        len(test_df[test_df[\"id\"] == id]) >= sequence_length\n",
        "        for id in test_df[\"id\"].unique()\n",
        "    ]\n",
        "    label_array_test_last = test_df.groupby(\"id\")[\"R_early\"].nth(-1)[y_mask].values\n",
        "    label_array_test_last = label_array_test_last.reshape(\n",
        "        label_array_test_last.shape[0], 1\n",
        "    ).astype(np.float32)\n",
        "    return seq_array_test_last, label_array_test_last\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKIeJ46SvVwx",
        "outputId": "c8bdb6fd-ed4c-4694-a5ef-d5d71b314b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Shape:  (74, 90, 3)\n",
            "Label Shape:  (74, 1)\n"
          ]
        }
      ],
      "source": [
        "#Specify test sample, test label\n",
        "seq_array_test, label_array_test = test_data_eval(test_df, sequence_length, sequence_cols)\n",
        "print(\"Data Shape: \", seq_array_test.shape)\n",
        "print(\"Label Shape: \", label_array_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "9tHNGc3m3fhk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "timesteps =  90 \n",
            "features =  3 \n",
            "output_length =  1\n"
          ]
        }
      ],
      "source": [
        "#Specify number of timesteps, features, and output\n",
        "nb_timesteps=seq_array.shape[1]\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "print(\"timesteps = \", nb_timesteps, \"\\nfeatures = \", nb_features, \"\\noutput_length = \", nb_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "FK2IfBNGTvFs"
      },
      "outputs": [],
      "source": [
        "#For conventional models, seq_array, will be used for training, and seq_array_test, will be used for testing\n",
        "#However, for multihead models, reshape the training sample so that each head is used to learn different features in the training sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Vv79NNYEdxMz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_sample_multihead = [seq_array[:,:,i].reshape((seq_array.shape[0],sequence_length,1)) for i in range(nb_features)]\n",
        "len(training_sample_multihead)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "b3bOsbCkYGYV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_sample_multihead = test_data = [seq_array_test[:,:,i].reshape((seq_array_test.shape[0],sequence_length,1)) for i in range(nb_features)]\n",
        "len(test_sample_multihead[0][73])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TuxTe_n_NzaC"
      },
      "outputs": [],
      "source": [
        "# these are the evaluation metrics for the model\n",
        "def k_score(y_true, y_pred):\n",
        "\n",
        "    \"\"\"score metric used for model evaluation\"\"\"\n",
        "\n",
        "    d = y_pred - y_true\n",
        "    return K.sum(K.exp(d[d >= 0] / 10) - 1) + K.sum(K.exp(-1 * d[d < 0] / 13) - 1)\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "\n",
        "    \"\"\"rmse metric used for model evaluation\"\"\"\n",
        "\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the new loss function\n",
        "\n",
        "def newLoss(y_true, y_pred) :\n",
        "\n",
        "    d = y_pred - y_true\n",
        "\n",
        "    return 0.95 * (K.mean(K.square(y_pred - y_true))) + 0.05 * (K.sum(K.exp(d[d >= 0] / 10) - 1) + K.sum(K.exp(-1 * d[d < 0] / 13) - 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb_aUdpBUX4G"
      },
      "source": [
        "#Create attention_based Multihead CNLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.config = {\n",
        "  \"learning_rate\": 3e-5,\n",
        "  \"epochs\": 30,\n",
        "  \n",
        "}\n",
        "\n",
        "# ... Define a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "2uGwtcobKW4r"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAMCNGRU(tested not good)\\n'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Attention_based multihead CNLSTM\n",
        "\"\"\"\n",
        "#This is my implemetation of the AMCNLSTM. Feel free to experiment with the architecture\n",
        "\n",
        "# in_layers, out_layers = list(), list()\n",
        "# for i in range(nb_features):\n",
        "#   inputs = Input(shape=(sequence_length,1))\n",
        "#   rnn1 = Conv1D(filters=64,kernel_size=2,strides=1,padding=\"same\")(inputs)\n",
        "#   lr1= LeakyReLU()(rnn1)\n",
        "#   bn1= BatchNormalization()(lr1)\n",
        "#   rnn2 = Conv1D(filters=64,kernel_size=2,strides=1,padding=\"same\")(bn1)\n",
        "#   lr2= LeakyReLU()(rnn2)\n",
        "#   bn2= BatchNormalization()(lr2)\n",
        "#   rnn3 = LSTM(units=50, return_sequences=True)(bn2)\n",
        "#   lr3= LeakyReLU()(rnn3)\n",
        "#   bn3= BatchNormalization()(lr3)\n",
        "#   att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, kernel_regularizer=keras.regularizers.l2(1e-4), bias_regularizer=keras.regularizers.l1(1e-4), attention_regularizer_weight=1e-4,attention_width=15)(bn3)\n",
        "#   pool1 = MaxPooling1D(pool_size=2)(att)\n",
        "#   flat = Flatten()(pool1)\n",
        "#   # store layers\n",
        "#   in_layers.append(inputs)\n",
        "#   out_layers.append(flat)\n",
        "# # merge heads\n",
        "# merged = concatenate(out_layers)\n",
        "# # interpretation\n",
        "# dense1 = Dense(50, activation='relu')(merged)\n",
        "# outputs = Dense(nb_out)(dense1)\n",
        "# model = Model(inputs=in_layers, outputs=outputs)\n",
        "# optimizer = keras.optimizers.adam_v2.Adam(lr=3e-5)\n",
        "\n",
        "# model.compile(loss=keras.losses.Huber(),\n",
        "#               optimizer=optimizer,\n",
        "#               metrics=[rmse, k_score])\n",
        "\n",
        "# AMCNLSTM = model\n",
        "# pydot_ng.find_graphviz()\n",
        "# plot_model(AMCNLSTM, to_file='AMCNLSTM-90.png', show_shapes=True, rankdir=\"TB\")\n",
        "\n",
        "\"\"\"\n",
        "AMBiLSTM model (testing)\n",
        "\"\"\"\n",
        "\n",
        "# in_layers, out_layers = list(), list()\n",
        "# for i in range(nb_features):\n",
        "#   inputs = Input(shape=(sequence_length, 1))\n",
        "#   rnn1 = Bidirectional(keras.layers.LSTM(units=50, return_sequences=True),merge_mode = 'concat')(inputs)\n",
        "#   lr1= LeakyReLU()(rnn1)\n",
        "#   bn1= BatchNormalization()(lr1)\n",
        "#   # att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, kernel_regularizer=keras.regularizers.l2(1e-4), bias_regularizer=keras.regularizers.l1(1e-4), attention_regularizer_weight=1e-4,attention_width=15)(bn1)\n",
        "#   pool1 = MaxPooling1D(pool_size=2)(bn1)\n",
        "#   flat = Flatten()(pool1)\n",
        "#   # store layers\n",
        "#   in_layers.append(inputs)\n",
        "#   out_layers.append(flat)\n",
        "# # merge heads\n",
        "# merged = concatenate(out_layers)\n",
        "# # interpretation\n",
        "# dense1 = Dense(50, activation='relu')(merged)\n",
        "# outputs = Dense(nb_out)(dense1)\n",
        "# model = Model(inputs=in_layers, outputs=outputs)\n",
        "# optimizer = keras.optimizers.adam_v2.Adam(lr=3e-5)\n",
        "\n",
        "# # model.compile(loss='mse',\n",
        "# #               optimizer=optimizer,\n",
        "# #               metrics=[rmse, k_score])\n",
        "# model.compile(loss=keras.losses.Huber(),\n",
        "#               optimizer=optimizer,\n",
        "#               metrics=[rmse, k_score])\n",
        "\n",
        "# AMBiLSTM = model\n",
        "# # ABiLSTM.summary()\n",
        "# # pydot_ng.find_graphviz()\n",
        "# # plot_model(AMBiLSTM, to_file='MBiLSTM.png', show_shapes=True, rankdir=\"TB\")\n",
        "\n",
        "\"\"\"\n",
        "AMCNGRU(tested not good)\n",
        "\"\"\"\n",
        "# from keras.layers.recurrent import GRU\n",
        "# in_layers, out_layers = list(), list()\n",
        "# for i in range(nb_features):\n",
        "#   inputs = Input(shape=(sequence_length,1))\n",
        "#   rnn3 = Bidirectional(GRU(units=50, recurrent_activation = 'sigmoid', reset_after=True, return_sequences=True))(inputs)\n",
        "#   lr3= LeakyReLU()(rnn3)\n",
        "#   bn3= BatchNormalization()(lr3)\n",
        "#   att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, kernel_regularizer=keras.regularizers.l2(1e-4), bias_regularizer=keras.regularizers.l1(1e-4), attention_regularizer_weight=1e-4,attention_width=15)(bn3)\n",
        "#   pool1 = MaxPooling1D(pool_size=2)(att)\n",
        "#   flat = Flatten()(pool1)\n",
        "#   # store layers\n",
        "#   in_layers.append(inputs)\n",
        "#   out_layers.append(flat)\n",
        "# # merge heads\n",
        "# merged = concatenate(out_layers)\n",
        "# # interpretation\n",
        "# dense1 = Dense(50, activation='relu')(merged)\n",
        "# outputs = Dense(nb_out)(dense1)\n",
        "# model = Model(inputs=in_layers, outputs=outputs)\n",
        "# optimizer = keras.optimizers.adam_v2.Adam(lr=3e-5)\n",
        "\n",
        "# model.compile(loss=keras.losses.Huber(),\n",
        "#               optimizer=optimizer,\n",
        "#               metrics=[rmse, k_score])\n",
        "\n",
        "# AMBiGRU = model\n",
        "# pydot_ng.find_graphviz()\n",
        "# plot_model(AMBiGRU, to_file='AMBiGRU.png', show_shapes=True, rankdir=\"TB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/chengyingzhu/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 90, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_19 (InputLayer)           [(None, 90, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           [(None, 90, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_70 (Dense)                (None, 90, 50)       100         input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 90, 50)       100         input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_78 (Dense)                (None, 90, 50)       100         input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_68 (LeakyReLU)      (None, 90, 50)       0           dense_70[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_72 (LeakyReLU)      (None, 90, 50)       0           dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_76 (LeakyReLU)      (None, 90, 50)       0           dense_78[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 90, 50)       200         leaky_re_lu_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 90, 50)       200         leaky_re_lu_72[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 90, 50)       200         leaky_re_lu_76[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_71 (Dense)                (None, 90, 50)       2550        batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 90, 50)       2550        batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_79 (Dense)                (None, 90, 50)       2550        batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_69 (LeakyReLU)      (None, 90, 50)       0           dense_71[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_73 (LeakyReLU)      (None, 90, 50)       0           dense_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_77 (LeakyReLU)      (None, 90, 50)       0           dense_79[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 90, 50)       200         leaky_re_lu_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 90, 50)       200         leaky_re_lu_73[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 90, 50)       200         leaky_re_lu_77[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 90, 50)       2550        batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_76 (Dense)                (None, 90, 50)       2550        batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_80 (Dense)                (None, 90, 50)       2550        batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_70 (LeakyReLU)      (None, 90, 50)       0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_74 (LeakyReLU)      (None, 90, 50)       0           dense_76[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_78 (LeakyReLU)      (None, 90, 50)       0           dense_80[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 90, 50)       200         leaky_re_lu_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 90, 50)       200         leaky_re_lu_74[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 90, 50)       200         leaky_re_lu_78[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 90, 50)       2550        batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_77 (Dense)                (None, 90, 50)       2550        batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_81 (Dense)                (None, 90, 50)       2550        batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_71 (LeakyReLU)      (None, 90, 50)       0           dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_75 (LeakyReLU)      (None, 90, 50)       0           dense_77[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_79 (LeakyReLU)      (None, 90, 50)       0           dense_81[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 90, 50)       200         leaky_re_lu_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 90, 50)       200         leaky_re_lu_75[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 90, 50)       200         leaky_re_lu_79[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_17 (MaxPooling1D) (None, 45, 50)       0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_18 (MaxPooling1D) (None, 45, 50)       0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_19 (MaxPooling1D) (None, 45, 50)       0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 2250)         0           max_pooling1d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_18 (Flatten)            (None, 2250)         0           max_pooling1d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_19 (Flatten)            (None, 2250)         0           max_pooling1d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 2250)         0           flatten_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 2250)         0           flatten_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 2250)         0           flatten_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 6750)         0           dropout_17[0][0]                 \n",
            "                                                                 dropout_18[0][0]                 \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_82 (Dense)                (None, 50)           337550      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_83 (Dense)                (None, 1)            51          dense_82[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 363,251\n",
            "Trainable params: 362,051\n",
            "Non-trainable params: 1,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nM2LSTM\\n'"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "MFNN model (testing)\n",
        "\"\"\"\n",
        "in_layers, out_layers = list(), list()\n",
        "for i in range(nb_features):\n",
        "  inputs = Input(shape=(sequence_length, 1))\n",
        "  den1 = Dense(units=50, activation='relu')(inputs)\n",
        "  lr1= LeakyReLU()(den1)\n",
        "  bn1= BatchNormalization()(lr1)\n",
        "  den2 = Dense(units=50, activation='relu')(bn1)\n",
        "  lr2= LeakyReLU()(den2)\n",
        "  bn2= BatchNormalization()(lr2)\n",
        "  den3 = Dense(units=50, activation='relu')(bn2)\n",
        "  lr3= LeakyReLU()(den3)\n",
        "  bn3= BatchNormalization()(lr3)\n",
        "  den4 = Dense(units=50, activation='relu')(bn3)\n",
        "  lr4= LeakyReLU()(den4)\n",
        "  bn4= BatchNormalization()(lr4)\n",
        "  # att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, kernel_regularizer=keras.regularizers.l2(1e-4), bias_regularizer=keras.regularizers.l1(1e-4), attention_regularizer_weight=1e-4,attention_width=15)(bn4)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(bn4)\n",
        "  flat = Flatten()(pool1)\n",
        "  drop = Dropout(0.2)(flat)\n",
        "  # store layers\n",
        "  in_layers.append(inputs)\n",
        "  out_layers.append(drop)\n",
        "# merge heads\n",
        "merged = concatenate(out_layers)\n",
        "# interpretation\n",
        "dense1 = Dense(50, activation='relu')(merged)\n",
        "outputs = Dense(nb_out)(dense1)\n",
        "model = Model(inputs=in_layers, outputs=outputs)\n",
        "optimizer = keras.optimizers.adam_v2.Adam(lr=3e-5)\n",
        "model.compile(loss = keras.losses.Huber(),\n",
        "              optimizer = optimizer,\n",
        "              metrics = [rmse, k_score])\n",
        "# model.compile(loss = newLoss,\n",
        "#               optimizer = optimizer,\n",
        "#               metrics = [rmse, k_score])\n",
        "\n",
        "MFNN4 = model\n",
        "model.summary()\n",
        "\n",
        "pydot_ng.find_graphviz()\n",
        "plot_model(MFNN4, to_file='MMLPs.png', show_shapes=True, rankdir=\"TB\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "M2LSTM\n",
        "\"\"\"\n",
        "\n",
        "# in_layers, out_layers = list(), list()\n",
        "# for i in range(nb_features):\n",
        "#   inputs = Input(shape=(sequence_length, 1))\n",
        "#   rnn1 = LSTM(units=64, return_sequences=True)(inputs)\n",
        "# #   rnn3 = Bidirectional(keras.layers.LSTM(units=50, return_sequences=True),merge_mode = 'concat')(inputs)\n",
        "#   lr1= LeakyReLU()(rnn1)\n",
        "#   bn1= BatchNormalization()(lr1)\n",
        "#   pool1 = MaxPooling1D(pool_size=2)(bn1)\n",
        "#   # flat = Flatten()(pool1)\n",
        "#   # store layers\n",
        "#   in_layers.append(inputs)\n",
        "#   out_layers.append(pool1)\n",
        "# # merge heads\n",
        "# merged = concatenate(out_layers)\n",
        "# # interpretation(add attention)\n",
        "\n",
        "# att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, kernel_regularizer=keras.regularizers.l2(1e-4), bias_regularizer=keras.regularizers.l1(1e-4), attention_regularizer_weight=1e-4,attention_width=15)(merged)\n",
        "# rnn2 = LSTM(units=64, return_sequences=True)(att)\n",
        "# dp = Dropout(0.2)(rnn2)\n",
        "# flat = Flatten()(dp)\n",
        "# outputs = Dense(nb_out)(flat)\n",
        "# model = Model(inputs=in_layers, outputs=outputs)\n",
        "# optimizer = keras.optimizers.adam_v2.Adam(lr=3e-5)\n",
        "# model.compile(loss = keras.losses.Huber(),\n",
        "#               optimizer = optimizer,\n",
        "#               metrics = [rmse, k_score])\n",
        "# # model.compile(loss=newLoss,\n",
        "# #               optimizer=optimizer,\n",
        "# #               metrics=[rmse, k_score])\n",
        "\n",
        "# MLSTMs = model\n",
        "# # ABiLSTM.summary()\n",
        "# pydot_ng.find_graphviz()\n",
        "# plot_model(MLSTMs, to_file='M2LSTM.png', show_shapes=True, rankdir=\"TB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grK7MSfYEsjB",
        "outputId": "621335de-67f3-44f2-9591-cc5f0b7eed2d",
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-12 00:46:26.217341: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
            "2022-06-12 00:46:26.217353: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
            "2022-06-12 00:46:26.217457: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
          ]
        }
      ],
      "source": [
        "# Creat model checkpoint to save the best model performance\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"content/sample_data/\" + filename + \".h5\", save_best_only=True, verbose=1\n",
        ")\n",
        "\n",
        "# specify ealy_callback to stop training model if there's no improvement after 5 epochs\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=5)\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='/output/Graph', histogram_freq=0, write_graph=True, write_images=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "Jwfbum36fyDG",
        "outputId": "e6931434-2970-412c-ce35-ee64f1aecb1a"
      },
      "outputs": [],
      "source": [
        "# Plot the model\n",
        "#It can be seen here that the number of heads= number of features (columns) in the training sample\n",
        "# pydot.find_graphviz()\n",
        "# keras.utils.plot_model(model, show_shapes=True, to_file='AMCNLSTM.png',dpi=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1y8R9FgXdNm",
        "outputId": "24b2a13b-0e2b-4483-eaa4-8d265add7f13",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-12 00:46:27.670805: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2022-06-12 00:46:27.671636: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-12 00:46:28.468131: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "290/291 [============================>.] - ETA: 0s - loss: 29.4888 - rmse: 38.4100 - k_score: 10227.8369"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-12 00:46:37.697591: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "291/291 [==============================] - 11s 32ms/step - loss: 29.4677 - rmse: 38.3687 - k_score: 10193.8623 - val_loss: 69.5468 - val_rmse: 72.9566 - val_k_score: 163530.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 69.54684, saving model to content/sample_data/FD001.h5\n",
            "Epoch 2/30\n",
            "291/291 [==============================] - 11s 37ms/step - loss: 19.0497 - rmse: 25.7082 - k_score: 721.5134 - val_loss: 33.5041 - val_rmse: 35.0025 - val_k_score: 3123.6331\n",
            "\n",
            "Epoch 00002: val_loss improved from 69.54684 to 33.50411, saving model to content/sample_data/FD001.h5\n",
            "Epoch 3/30\n",
            " 39/291 [===>..........................] - ETA: 7s - loss: 17.7062 - rmse: 23.7896 - k_score: 502.2395"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/g5/v9d80nnn5g5fbhhf8h2q_n9c0000gn/T/ipykernel_19660/2142557642.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train the multihead model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         train_history=model.fit(training_sample_multihead, label_array, batch_size=32, epochs=30,\n\u001b[0m\u001b[1;32m      4\u001b[0m         validation_split=0.2, callbacks=[model_checkpoint,WandbCallback()])\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Network error (ConnectionError), entering retry loop.\n",
            "wandb: Network error (ReadTimeout), entering retry loop.\n"
          ]
        }
      ],
      "source": [
        "#Train the multihead model\n",
        "with tf.device('/cpu:0'):\n",
        "        train_history=model.fit(training_sample_multihead, label_array, batch_size=32, epochs=30,\n",
        "        validation_split=0.2, callbacks=[model_checkpoint,WandbCallback()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlgoXl1TfyDG"
      },
      "outputs": [],
      "source": [
        "# load the best checkpoint and evaluate on test set\n",
        "\n",
        "# This function loads the best model\n",
        "def load_model_from_disk(filename):\n",
        "\n",
        "    \"\"\"Function to load the best model after training\"\"\"\n",
        "\n",
        "    return load_model(\n",
        "        \"content/sample_data/\" + filename + \".h5\",\n",
        "        # custom_objects={\"rmse\": rmse, \"k_score\": k_score,\"SeqSelfAttention\":SeqSelfAttention, \"newLoss\": newLoss})\n",
        "        custom_objects={\"rmse\": rmse, \"k_score\": k_score,\"SeqSelfAttention\":SeqSelfAttention})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XqGQ28qap_I"
      },
      "outputs": [],
      "source": [
        "trained_model = load_model_from_disk(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O09JAp5MfyDG",
        "outputId": "b4d7a167-b97f-4df2-ea60-66e713e701be"
      },
      "outputs": [],
      "source": [
        "#Evaluate the model on test data\n",
        "with tf.device('/cpu:0'):\n",
        "    model_evaluation_res = trained_model.evaluate(\n",
        "        test_sample_multihead, label_array_test, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4PT5L0vfyDG"
      },
      "outputs": [],
      "source": [
        "# Predict the degradation trend in test data\n",
        "y_pred = trained_model.predict(test_data)\n",
        "#y_pred=np.squeeze(y_pred)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbp27wgmfykB"
      },
      "outputs": [],
      "source": [
        "#plot the prediction vs the ground truth\n",
        "def plot_result(y_true, y_pred, title=None):\n",
        "\n",
        "    \"\"\" A function to plot the predicted RUL vs the target RUL for all engines\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize = (10.5, 9.5))\n",
        "    plt.grid(True,lw=0.1)\n",
        "    plt.rcParams['axes.linewidth'] = 0.4  # set the value globally\n",
        "    plt.plot(y_true, 'r', linewidth=1,linestyle='--')\n",
        "    plt.plot(y_pred, 'k', linewidth=1)\n",
        "    plt.title(title,fontsize=8)\n",
        "    plt.xlabel('Engine', fontsize=9)\n",
        "    plt.xticks(fontsize=5)\n",
        "    plt.ylabel('RUL', fontsize=7)\n",
        "    plt.yticks(fontsize=5)\n",
        "    plt.legend(['Actual RUL', 'Predicted RUL'],  loc='upper right', fontsize=5)\n",
        "    plt.show()\n",
        "    fig.savefig(\"content/sample_data/\"  + title + '.png', dpi=256, format='png', bbox_inches='tight')\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "Dsw-kCKRSfcy",
        "outputId": "216870c7-c887-4778-ad9f-043efbf16c64"
      },
      "outputs": [],
      "source": [
        "# View the results\n",
        "plot_result(\n",
        "    label_array_test, y_pred, title=\"Model MFNN prediction on \" + filename + \" test set\"\n",
        ")\n",
        "\n",
        "print (\"sequence length = \", sequence_length ,\"\\nRMSE = \", K.sqrt(K.mean(K.square(label_array_test - y_pred))))\n",
        "from sklearn.metrics import r2_score\n",
        "print ('R2-score = ', round(r2_score(label_array_test,y_pred), 4))\n",
        "# print (\"Adjust_R2 = \", 1-((1-r2_score(label_array_test,y_pred)) * (seq_array_test.shape[0]-1))/(seq_array_test.shape[0]-seq_array_test.shape[2]-1))\n",
        "\n",
        "\n",
        "per_real_loss=(label_array_test-y_pred)/label_array_test\n",
        "avg_per_real_loss=sum(abs(per_real_loss))/len(per_real_loss)\n",
        "print(\"avg_per_real_loss = \", avg_per_real_loss)\n",
        "\n",
        "def comput_acc(real,predict,level):\n",
        "    num_error=0\n",
        "    for i in range(len(real)):\n",
        "        if abs(real[i]-predict[i])/real[i]>level:\n",
        "            num_error+=1\n",
        "    return 1-num_error/len(real)\n",
        "print (\"不同置信区间内的准确率：\", \"0.2:\",comput_acc(label_array_test,y_pred,0.2),\n",
        "\"0.15:\", comput_acc(label_array_test,y_pred,0.15),\"0.1:\", comput_acc(label_array_test,y_pred,0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "3iKhsWtGfG2h",
        "outputId": "b8b336f4-1110-4d03-d024-fb8b5cf07143"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Test the model on randomly selected engine from the test dataset\n",
        "\n",
        "# if filename == \"FD001\":\n",
        "#     eng_no = [5,21,31,64]\n",
        "# elif filename ==\"FD002\":\n",
        "#     eng_no= [11,31]\n",
        "# elif filename==\"FD003\":\n",
        "#     eng_no=[4,10,34,71,96]\n",
        "# else:\n",
        "#     eng_no= [1,37]   \n",
        "\n",
        "# for numb in eng_no:\n",
        "eng_df = pd.read_csv(\n",
        "    \"random_test/random_eng_FD001-21.csv\"\n",
        ")\n",
        "seq_array_eng = sequence_gen(eng_df, sequence_length, sequence_cols)\n",
        "seq_array_eng2 = [seq_array_eng[:,:,i].reshape((seq_array_eng.shape[0],sequence_length,1)) for i in range(nb_features)]\n",
        "print(seq_array_eng.shape)\n",
        "label_array_eng = label_generator(eng_df, sequence_length, [\"R_early\"])\n",
        "print(label_array_eng.shape)\n",
        "y_pred_eng = trained_model.predict(seq_array_eng2)\n",
        "#y_pred_eng = best_model.predict(seq_array_eng)\n",
        "y_pred_eng=np.squeeze(y_pred_eng)\n",
        "plot_result(\n",
        "    label_array_eng,\n",
        "    y_pred_eng,\n",
        "    title=\"Test engine unit 5\" + filename\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Mult-head implementaton.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "9e6528faff5e9f23aa93b34b1dcf641ed40d9e33c09e0ac13b5e136c8ee5597e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
